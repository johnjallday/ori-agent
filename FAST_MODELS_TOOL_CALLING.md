# Fast Models for Tool Calling

Yes! There are several **fast, small models** optimized for tool calling.

## üèÜ Top Recommendations (Fast + Small + Good Tool Calling)

### 1. **Mistral 7B** ‚≠ê‚≠ê‚≠ê **BEST OVERALL**
```bash
ollama pull mistral
OLLAMA_MODEL=mistral make test-ollama
```
- **Size**: ~4GB
- **Speed**: Fast ‚ö°
- **Tool Calling**: ‚úÖ Excellent
- **RAM**: 8GB+
- **Why**: Best balance of size, speed, and tool calling quality

### 2. **Phi-4-Mini** ‚≠ê‚≠ê **FASTEST**
```bash
ollama pull phi4-mini
OLLAMA_MODEL=phi4-mini make test-ollama
```
- **Size**: ~2.5GB (3.8B parameters)
- **Speed**: Very Fast ‚ö°‚ö°
- **Tool Calling**: ‚úÖ Good
- **RAM**: 4GB+
- **Why**: Smallest model with solid tool calling

### 3. **Nemotron-Mini** ‚≠ê **OPTIMIZED FOR FUNCTION CALLING**
```bash
ollama pull nemotron-mini
OLLAMA_MODEL=nemotron-mini make test-ollama
```
- **Size**: ~2.7GB (4B parameters)
- **Speed**: Very Fast ‚ö°‚ö°
- **Tool Calling**: ‚úÖ Excellent (optimized for it!)
- **RAM**: 6GB+
- **Why**: Specifically optimized for function calling

### 4. **Qwen 2.5 (3B)** ‚≠ê **MULTILINGUAL + FAST**
```bash
ollama pull qwen2.5:3b
OLLAMA_MODEL=qwen2.5:3b make test-ollama
```
- **Size**: ~2GB (3B parameters)
- **Speed**: Very Fast ‚ö°‚ö°
- **Tool Calling**: ‚úÖ Good
- **RAM**: 4GB+
- **Why**: Very small, multilingual, 128K context

---

## üìä Performance Comparison

| Model | Size | Speed | Tool Calling | RAM | Best For |
|-------|------|-------|--------------|-----|----------|
| **mistral** | 4GB | ‚ö°‚ö° Fast | ‚úÖ Excellent | 8GB | **General use** ‚≠ê‚≠ê‚≠ê |
| **phi4-mini** | 2.5GB | ‚ö°‚ö°‚ö° Very Fast | ‚úÖ Good | 4GB | **Speed priority** ‚≠ê‚≠ê |
| **nemotron-mini** | 2.7GB | ‚ö°‚ö°‚ö° Very Fast | ‚úÖ Excellent | 6GB | **Function calling** ‚≠ê‚≠ê |
| **qwen2.5:3b** | 2GB | ‚ö°‚ö°‚ö° Very Fast | ‚úÖ Good | 4GB | **Low memory** ‚≠ê |
| **granite 3.2:3b** | 2GB | ‚ö°‚ö°‚ö° Very Fast | ‚úÖ Good | 4GB | **Long context** ‚≠ê |
| llama3.1:8b | 4.9GB | ‚ö°‚ö° Fast | ‚ùå Poor | 8GB | ‚ùå Not recommended |
| mixtral | 4.7GB | ‚ö° Medium | ‚úÖ Excellent | 16GB | Slower but accurate |
| llama3.1:70b | 40GB | üêå Very Slow | ‚úÖ Good | 64GB | ‚ùå Too slow |

---

## üöÄ Quick Test Commands

### Test with Mistral (Recommended)
```bash
ollama pull mistral
OLLAMA_MODEL=mistral make test-ollama
```

### Test with Phi-4-Mini (Fastest)
```bash
ollama pull phi4-mini
OLLAMA_MODEL=phi4-mini make test-ollama
```

### Test with Nemotron-Mini (Function Calling Optimized)
```bash
ollama pull nemotron-mini
OLLAMA_MODEL=nemotron-mini make test-ollama
```

### Test with Qwen 2.5 (Smallest)
```bash
ollama pull qwen2.5:3b
OLLAMA_MODEL=qwen2.5:3b make test-ollama
```

---

## üìà Detailed Model Profiles

### Mistral 7B - Best All-Around
**Perfect for:** Production local deployment

```bash
ollama pull mistral
```

**Stats:**
- Parameters: 7B
- Size: ~4GB
- Context: 32K tokens
- Strengths: Excellent tool calling, good reasoning, fast inference
- Weaknesses: Needs 8GB+ RAM

**Expected Test Performance:**
- Agent creation: ~1-2s
- Tool calling: ~3-5s per call
- Full test suite: ~5-8 minutes

---

### Phi-4-Mini - Fastest Compact
**Perfect for:** Resource-constrained environments

```bash
ollama pull phi4-mini
```

**Stats:**
- Parameters: 3.8B
- Size: ~2.5GB
- Context: 16K tokens
- Strengths: Very fast, low memory, good math/reasoning
- Weaknesses: Smaller context window

**Expected Test Performance:**
- Agent creation: ~0.5-1s
- Tool calling: ~1-3s per call
- Full test suite: ~3-5 minutes

---

### Nemotron-Mini - Function Calling Specialist
**Perfect for:** Tool-heavy applications

```bash
ollama pull nemotron-mini
```

**Stats:**
- Parameters: 4B
- Size: ~2.7GB
- Context: 8K tokens
- Strengths: **Optimized for function calling**, roleplay, RAG
- Weaknesses: Smaller context window

**Expected Test Performance:**
- Agent creation: ~1s
- Tool calling: ~2-4s per call (very reliable)
- Full test suite: ~4-6 minutes

---

### Qwen 2.5 (3B) - Multilingual Compact
**Perfect for:** Low-memory systems, multilingual needs

```bash
ollama pull qwen2.5:3b
```

**Stats:**
- Parameters: 3B
- Size: ~2GB
- Context: 128K tokens (huge!)
- Strengths: Tiny size, multilingual, massive context
- Weaknesses: May be less accurate than larger models

**Expected Test Performance:**
- Agent creation: ~0.5-1s
- Tool calling: ~2-4s per call
- Full test suite: ~3-5 minutes

---

## üéØ Which Model Should You Use?

### For Production Testing
```bash
ollama pull mistral
OLLAMA_MODEL=mistral make test-ollama
```
**Why:** Best balance of speed, quality, and reliability

### For Development/Quick Tests
```bash
ollama pull phi4-mini
OLLAMA_MODEL=phi4-mini make test-ollama
```
**Why:** Fastest, uses least RAM, good enough for dev work

### For Tool-Heavy Workflows
```bash
ollama pull nemotron-mini
OLLAMA_MODEL=nemotron-mini make test-ollama
```
**Why:** Specifically optimized for function calling

### For Low-Memory Systems
```bash
ollama pull qwen2.5:3b
OLLAMA_MODEL=qwen2.5:3b make test-ollama
```
**Why:** Only 2GB, runs on 4GB RAM systems

---

## ‚ö° Speed Benchmarks (Estimated)

### Complete Test Suite (~18 tests)

| Model | Total Time | Per Test Avg |
|-------|-----------|--------------|
| **phi4-mini** | ~3-5 min | ~10-15s | ‚ö°‚ö°‚ö° Fastest
| **nemotron-mini** | ~4-6 min | ~15-20s | ‚ö°‚ö°‚ö° Very Fast
| **qwen2.5:3b** | ~3-5 min | ~10-15s | ‚ö°‚ö°‚ö° Very Fast
| **mistral** | ~5-8 min | ~20-30s | ‚ö°‚ö° Fast
| mixtral | ~10-15 min | ~40-60s | ‚ö° Medium
| llama3.1:8b | ~6-10 min | ~20-30s | ‚ö°‚ö° Fast (but tools fail)
| llama3.1:70b | ~30-60 min | ~2-4min | üêå Very Slow
| **gpt-4o-mini** | ~2-3 min | ~8-12s | ‚ö°‚ö°‚ö°‚ö° Cloud (fastest)

---

## üí° Pro Tips

### 1. Use Quantized Models for Speed
```bash
# 4-bit quantized (faster, less accurate)
ollama pull mistral:7b-q4_0

# 8-bit quantized (balanced)
ollama pull mistral:7b-q8_0
```

### 2. Adjust Context Window
Smaller context = faster inference:
```bash
ollama run mistral --ctx-size 2048  # Faster
ollama run mistral --ctx-size 32768 # Slower but better
```

### 3. Run Specific Tests Only
```bash
# Test one workflow (fast)
USE_OLLAMA=true OLLAMA_MODEL=phi4-mini \
  go test -v ./tests/user/workflows/... -run TestAgentWithPluginWorkflow

# Test math plugin only (fast)
USE_OLLAMA=true OLLAMA_MODEL=nemotron-mini \
  go test -v ./tests/user/plugins/... -run TestMathPluginIntegration
```

### 4. Use Verbose Mode to See Progress
```bash
TEST_VERBOSE=true OLLAMA_MODEL=mistral make test-ollama
```

---

## üî• Recommended Setup

### For Daily Development
```bash
# Install Phi-4-Mini (fastest for quick iterations)
ollama pull phi4-mini

# Quick test
OLLAMA_MODEL=phi4-mini \
  go test -v ./tests/user/workflows/... -run TestAgentWithPluginWorkflow
```

### For CI/CD Pipeline
```bash
# Use cloud APIs for speed and reliability
export OPENAI_API_KEY="your-key"
make test-user  # ~2-3 minutes total
```

### For Comprehensive Local Testing
```bash
# Install Mistral (best local quality)
ollama pull mistral

# Full test suite
OLLAMA_MODEL=mistral make test-ollama  # ~5-8 minutes
```

---

## üé™ Real-World Example

### Phi-4-Mini (Fast Test)
```bash
$ time OLLAMA_MODEL=phi4-mini go test -v ./tests/user/workflows/... -run TestAgentWithPluginWorkflow

=== RUN   TestAgentWithPluginWorkflow
    ‚úì Created agent: plugin-test-agent (model: phi4-mini)
    ‚úì Enabled plugin 'math'
    ‚úì Tool called: math (1.2s)
    ‚úì Response contains: '42'
--- PASS: TestAgentWithPluginWorkflow (4.3s)

real    0m4.8s    ‚Üê Fast! ‚ö°‚ö°‚ö°
```

### Mistral (Balanced)
```bash
$ time OLLAMA_MODEL=mistral go test -v ./tests/user/workflows/... -run TestAgentWithPluginWorkflow

=== RUN   TestAgentWithPluginWorkflow
    ‚úì Created agent: plugin-test-agent (model: mistral)
    ‚úì Enabled plugin 'math'
    ‚úì Tool called: math (3.1s)
    ‚úì Response contains: '42'
--- PASS: TestAgentWithPluginWorkflow (8.2s)

real    0m8.6s    ‚Üê Still fast! ‚ö°‚ö°
```

---

## üìö Summary

**Fastest + Good Tool Calling:**
1. **Phi-4-Mini** (2.5GB) - Best for speed
2. **Nemotron-Mini** (2.7GB) - Best for function calling
3. **Qwen 2.5 3B** (2GB) - Best for low memory

**Best Overall:**
- **Mistral 7B** (4GB) - Best balance

**For Production:**
- **GPT-4o-mini** (Cloud) - Fastest + most reliable

---

## üöÄ Get Started Now

```bash
# Install the fastest reliable model
ollama pull phi4-mini

# Run a quick test
OLLAMA_MODEL=phi4-mini \
  go test -v ./tests/user/workflows/... -run TestAgentWithPluginWorkflow

# If that works well, run full suite
OLLAMA_MODEL=phi4-mini make test-ollama
```

**Expected result:** ~3-5 minute full test suite with good tool calling! ‚ö°
